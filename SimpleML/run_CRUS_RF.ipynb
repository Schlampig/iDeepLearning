{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Basic models\n",
    "import traceback\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Data operation models\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import cross_validation  \n",
    "from sklearn.model_selection import ParameterGrid \n",
    "\n",
    "# Classifier models\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Evaluation models\n",
    "import sklearn.metrics as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Configurations for the model\n",
    "def run_Config():\n",
    "    data_name = 'Thoracic_Surgery_onehot.xlsx'\n",
    "    k_holdout = 10\n",
    "    k_cv = 5\n",
    "    origin_pars = {\n",
    "            'indicator_name': ['macc'],\n",
    "            'k_centers': [1,3,5], # crus_hyper-par\n",
    "            'n_estimators': [5,10,30,50],\n",
    "            'max_depth': [2,4,6,8],\n",
    "            'max_features': ['log2'],\n",
    "            'class_weight':[{1:0.1,2:0.9},{1:0.2,2:0.8},{1:0.25,2:0.75},{1:0.4,2:0.6}]\n",
    "            }\n",
    "    pars = list(ParameterGrid(origin_pars))\n",
    "    return data_name, k_holdout, k_cv, pars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "def run_load_data(data_name):\n",
    "    all = pd.read_excel(data_name)\n",
    "    all = all.values\n",
    "    all_fea = all[:,:-1]\n",
    "    all_label = get_normal_label(all[:,-1])\n",
    "    return all_fea, all_label\n",
    "\n",
    "def get_normal_label(y):\n",
    "    y_uni = np.unique(np.array(y))\n",
    "    for i in xrange(len(y_uni)):\n",
    "        y[np.nonzero(y == y_uni[i])[0]] = i+1\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Run both HoldoutCV and GridSearchCV\n",
    "def run_doubleCV(all_fea, all_label, k_holdout, k_cv, pars):\n",
    "    # HoldoutCV\n",
    "    i_t = 0\n",
    "    res_list = []\n",
    "    opt_pars_list = []\n",
    "    holdoutcv = StratifiedKFold(n_splits = k_holdout, shuffle = True)\n",
    "    for i_learn, i_test in holdoutcv.split(all_fea, all_label):\n",
    "        # Times\n",
    "        i_t = i_t + 1\n",
    "        # print 'Round ', str(i_t), ' Holdout CV----------------------'\n",
    "        \n",
    "        # Obtain current learning and heldout data\n",
    "        learn_fea, test_fea = all_fea[i_learn], all_fea[i_test]\n",
    "        learn_label, test_label = all_label[i_learn], all_label[i_test]\n",
    "        \n",
    "        # GridSearchCV\n",
    "        j_t = 0\n",
    "        optdata = {'score':0}\n",
    "        for i_pars in pars:\n",
    "            # times\n",
    "            j_t = j_t + 1\n",
    "            # print 'round ', str(j_t), ' gridsearch cv----------------'\n",
    "            pars_score = []\n",
    "            gridcv = StratifiedKFold(n_splits = k_cv, shuffle = True)\n",
    "            for i_train, i_valid in gridcv.split(learn_fea, learn_label):\n",
    "                # obtain current training and validation data\n",
    "                train_fea, valid_fea = learn_fea[i_train], learn_fea[i_valid]\n",
    "                train_label, valid_label = learn_label[i_train], learn_label[i_valid]\n",
    "                # learn the model\n",
    "                # i_pars = {'par_name1':par1,'par_name2':par2,...,'par_nameN':parN}\n",
    "                valid_pre, pars_new = run_crus_model(i_pars, train_fea, train_label, valid_fea, 0)\n",
    "                grid_score = run_validation(valid_pre, valid_label, i_pars['indicator_name'])\n",
    "                pars_score.append(grid_score)\n",
    "            if np.mean(pars_score) > optdata['score']:\n",
    "                optdata['pars'] = pars_new\n",
    "                optdata['score'] = np.mean(pars_score)\n",
    "                    \n",
    "        # Holdout testing\n",
    "        # best_pars is a dict too\n",
    "        best_pars = optdata['pars']\n",
    "        test_pre, _ = run_crus_model(best_pars, learn_fea, learn_label, test_fea, 1)\n",
    "        \n",
    "        # Evaluate the prediction\n",
    "        res_now = run_evaluation(test_pre, test_label)\n",
    "        \n",
    "        # Save results\n",
    "        res_list.append(res_now)\n",
    "        opt_pars_list.append(best_pars)\n",
    "        \n",
    "    return res_list, opt_pars_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Fit the model and make the prediction\n",
    "def run_crus_model(p, X, y, Z, i_tag):\n",
    "    if i_tag == 0:\n",
    "        # run the crus to get new training data\n",
    "        X_new, y_new, k_new = run_CRUS(X, y, p['k_centers'])\n",
    "        # update k_centers\n",
    "        p['k_centers'] = k_new\n",
    "    else:\n",
    "        X_new, y_new = X, y\n",
    "    \n",
    "    # run basic model\n",
    "    clf = RandomForestClassifier(n_estimators=p['n_estimators'], \n",
    "                                 max_depth=p['max_depth'], \n",
    "                                 max_features=p['max_features'], \n",
    "                                 class_weight=p['class_weight'])\n",
    "    clf.fit(X_new,y_new)\n",
    "    zpre = clf.predict(Z)\n",
    "    # print 'CRUS-RF is running...'\n",
    "    return zpre, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Undersampling by CRUS\n",
    "from sklearn.cluster import KMeans\n",
    "import math\n",
    "def run_CRUS(X, y, k):\n",
    "    \n",
    "    # obtain postive and negative data\n",
    "    v = np.unique(y)\n",
    "    if len(v) == 2:\n",
    "        if sum(y == v[0]) >= sum(y == v[1]):\n",
    "            X_pos = X[y == v[1],:]\n",
    "            X_neg = X[y == v[0],:]\n",
    "            y_pos = y[y == v[1]]\n",
    "            y_neg = y[y == v[0]]\n",
    "        else:\n",
    "            X_pos = X[y == v[0],:]\n",
    "            X_neg = X[y == v[1],:]\n",
    "            y_pos = y[y == v[0]]\n",
    "            y_neg = y[y == v[1]]\n",
    "    else:\n",
    "        raise Exception(\"Not a binary-class!\")    \n",
    "    n_pos = X_pos.shape[0]\n",
    "    n_neg = X_neg.shape[0]\n",
    "    \n",
    "    # constrain hyper-parameters to their suitable ranges\n",
    "    if n_pos == 0:\n",
    "        raise Exception(\"No positive samples!\")\n",
    "    else:\n",
    "        IR = n_neg/float(n_pos)\n",
    "    \n",
    "    # do the clustering for the negative smples\n",
    "    kclf = KMeans(n_clusters = k, max_iter = 1000)\n",
    "    kclf.fit(X_neg)\n",
    "    neg_center, neg_index = kclf.cluster_centers_, kclf.labels_\n",
    "        \n",
    "    # do the undersampling for the clustered negative samples\n",
    "    new_neg_mat = []\n",
    "    for i in xrange(k):\n",
    "        X_i = X_neg[np.nonzero(neg_index == i)[0], :]\n",
    "        n_i = X_i.shape[0]\n",
    "        c_i = int(n_i/IR)\n",
    "        index_i = range(n_i)\n",
    "        np.random.shuffle(index_i)\n",
    "        new_neg_mat.extend(X_i[index_i[:c_i],:])\n",
    "    new_neg_label = y_neg[0] * np.ones((len(new_neg_mat)))\n",
    "    \n",
    "    # combine the newly-generated ones to the original data\n",
    "    X_new = np.concatenate((X_pos, new_neg_mat), axis = 0)\n",
    "    y_new = np.concatenate((y_pos, new_neg_label), axis = 0)                                 \n",
    "    i_shuffle = np.random.permutation(len(y_new))\n",
    "    X_new = X_new[i_shuffle, :] \n",
    "    y_new = y_new[i_shuffle] \n",
    "    \n",
    "    return X_new, y_new, k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Obtain the score\n",
    "def run_validation(zpre, z, s_name):\n",
    "    i_pos = np.nonzero(z == max(z))\n",
    "    i_neg = np.nonzero(z == min(z))\n",
    "    tpr = (1 - sm.hamming_loss(z[i_pos], zpre[i_pos]))*100\n",
    "    tnr = (1 - sm.hamming_loss(z[i_neg], zpre[i_neg]))*100\n",
    "    if s_name.lower() == 'macc':\n",
    "        s = 0.5*(tpr+tnr)\n",
    "    elif s_name.lower() == 'gm':\n",
    "        s = np.sqrt(tpr*tnr)\n",
    "    elif s_name.lower() == 'tpr':\n",
    "        s = tpr\n",
    "    else: # error\n",
    "        s = sum((1 if i_pre == i_true else 0 for i_pre, i_true in zip(zpre,z)))/float(len(z))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Evaluate the performance\n",
    "def run_evaluation(p, y):\n",
    "    res_dict = {}\n",
    "    i_pos = np.nonzero(y == max(y))\n",
    "    i_neg = np.nonzero(y == min(y))\n",
    "    res_dict['TPR'] = (1 - sm.hamming_loss(y[i_pos], p[i_pos]))*100\n",
    "    res_dict['TNR'] = (1 - sm.hamming_loss(y[i_neg], p[i_neg]))*100\n",
    "    res_dict['MAcc'] = np.mean([res_dict['TPR'], res_dict['TNR']])\n",
    "    res_dict['GM'] = np.sqrt(res_dict['TPR']*res_dict['TNR'])\n",
    "    res_dict['F1(Macro)'] = sm.f1_score(y, p, average='macro')*100\n",
    "    res_dict['F1(Micro)'] = sm.f1_score(y, p, average='micro')*100\n",
    "    res_dict['Acc'] = sm.accuracy_score(y, p)*100\n",
    "    return res_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_dict(d):\n",
    "    list_final = []\n",
    "    for i in d:\n",
    "        list_now = i + '_' + str(d[i])\n",
    "        list_final.append(list_now)\n",
    "    return list_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    data_name, k_holdout, k_cv, pars = run_Config()\n",
    "    all_fea, all_label = run_load_data(data_name)\n",
    "    res_list, opt_pars_list = run_doubleCV(all_fea, all_label, k_holdout, k_cv, pars)\n",
    "    # print 'Each heldout cv result:'\n",
    "    # print '-----------------------'\n",
    "    for i, j in zip(res_list,opt_pars_list):\n",
    "        print get_dict(i), 'with hyper-parameters:'\n",
    "        print get_dict(j)\n",
    "        print '-----------------------'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
