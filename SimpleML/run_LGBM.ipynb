{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/tensorflow/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Basic models\n",
    "import traceback\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Data operation models\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import cross_validation\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "# Classifier models\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Evaluation models\n",
    "import sklearn.metrics as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Configurations for the model\n",
    "def run_Config():\n",
    "    data_name = 'Thoracic_Surgery_onehot.xlsx'\n",
    "    k_holdout = 10\n",
    "    k_cv = 5\n",
    "    origin_pars = {\n",
    "        'indicator_name': ['TPR','MAcc','F1(Micro)','F1(Macro)'],\n",
    "        'is_unbalance':['True'],\n",
    "        'application':['multiclass'],\n",
    "        'num_class':[2],\n",
    "        'boosting':['gbdt'],\n",
    "        'num_iterations':[100],\n",
    "        'learning_rate':[0.1],\n",
    "        'num_leaves':[31], \n",
    "        'num_trees':[5,10,30]\n",
    "    }\n",
    "    pars = list(ParameterGrid(origin_pars))\n",
    "    return data_name, k_holdout, k_cv, pars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "def run_load_data(data_name):\n",
    "    all = pd.read_excel(data_name)\n",
    "    all = all.values\n",
    "    all_fea = all[:,:-1]\n",
    "    all_label = get_normal_label(all[:,-1])\n",
    "    return all_fea, all_label\n",
    "\n",
    "def get_normal_label(y):\n",
    "    y_uni = np.unique(np.array(y))\n",
    "    for i in xrange(len(y_uni)):\n",
    "        y[np.nonzero(y == y_uni[i])[0]] = i\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Run both HoldoutCV and GridSearchCV\n",
    "def run_doubleCV(all_fea, all_label, k_holdout, k_cv, pars):\n",
    "    # HoldoutCV\n",
    "    i_t = 0\n",
    "    res_list = []\n",
    "    opt_pars_list = []\n",
    "    holdoutcv = StratifiedKFold(n_splits = k_holdout, shuffle = True)\n",
    "    for i_learn, i_test in holdoutcv.split(all_fea, all_label):\n",
    "        # Times\n",
    "        i_t = i_t + 1\n",
    "        # print 'Round ', str(i_t), ' Holdout CV----------------------'\n",
    "        \n",
    "        # Obtain current learning and heldout data\n",
    "        learn_fea, test_fea = all_fea[i_learn], all_fea[i_test]\n",
    "        learn_label, test_label = all_label[i_learn], all_label[i_test]\n",
    "        \n",
    "        # GridSearchCV\n",
    "        j_t = 0\n",
    "        optdata = {'score':0}\n",
    "        for i_pars in pars:\n",
    "            # times\n",
    "            j_t = j_t + 1\n",
    "            # print 'round ', str(j_t), ' gridsearch cv----------------'\n",
    "            pars_score = []\n",
    "            gridcv = StratifiedKFold(n_splits = k_cv, shuffle = True)\n",
    "            for i_train, i_valid in gridcv.split(learn_fea, learn_label):\n",
    "                # obtain current training and validation data\n",
    "                train_fea, valid_fea = learn_fea[i_train], learn_fea[i_valid]\n",
    "                train_label, valid_label = learn_label[i_train], learn_label[i_valid]\n",
    "                # learn the model\n",
    "                # i_pars = {'par_name1':par1,'par_name2':par2,...,'par_nameN':parN}\n",
    "                valid_pre, pars_new = run_LGBM(i_pars, train_fea, train_label, valid_fea)\n",
    "                eval_dict = run_evaluation(valid_pre, valid_label)\n",
    "                grid_score = eval_dict[i_pars['indicator_name']]\n",
    "                pars_score.append(grid_score)\n",
    "            if np.mean(pars_score) > optdata['score']:\n",
    "                optdata['pars'] = pars_new\n",
    "                optdata['score'] = np.mean(pars_score)\n",
    "                    \n",
    "        # Holdout testing\n",
    "        # best_pars is a dict too\n",
    "        best_pars = optdata['pars']\n",
    "        test_pre, _ = run_LGBM(best_pars, learn_fea, learn_label, test_fea)\n",
    "        \n",
    "        # Evaluate the prediction\n",
    "        res_now = run_evaluation(test_pre, test_label)\n",
    "        \n",
    "        # Save results\n",
    "        res_list.append(res_now)\n",
    "        opt_pars_list.append(best_pars)\n",
    "        \n",
    "    return res_list, opt_pars_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Run LightGBM\n",
    "def run_LGBM(p, X, y, Z):\n",
    "    # p-hyper-parameters in dict\n",
    "    # X-data\n",
    "    # y-label\n",
    "    # Z-data\n",
    "    # zpre-predicted label for Z\n",
    "    \n",
    "    # generate datasets and assign hyper-parameters\n",
    "    train_data = lgb.Dataset(X, label=y)\n",
    "    param = {\n",
    "        'is_unbalance':p['is_unbalance'],\n",
    "        'application':p['application'],\n",
    "        'num_class':p['num_class'],\n",
    "        'boosting':p['boosting'],\n",
    "        'num_iterations':p['num_iterations'],\n",
    "        'learning_rate':p['learning_rate'],\n",
    "        'num_leaves':p['num_leaves'], \n",
    "        'num_trees':p['num_trees']\n",
    "    }\n",
    "    \n",
    "    # train and test\n",
    "    clf = lgb.train(param, train_data)\n",
    "    zscore = clf.predict(Z)\n",
    "    zpre = np.argmax(zscore,axis=1)\n",
    "    # print 'zpre:', zpre\n",
    "    return zpre, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Evaluate the performance\n",
    "def run_evaluation(p, y):\n",
    "    res_dict = {}\n",
    "    i_pos = np.nonzero(y == max(y))\n",
    "    i_neg = np.nonzero(y == min(y))\n",
    "    res_dict['TPR'] = (1 - sm.hamming_loss(y[i_pos], p[i_pos]))*100\n",
    "    res_dict['TNR'] = (1 - sm.hamming_loss(y[i_neg], p[i_neg]))*100\n",
    "    res_dict['MAcc'] = np.mean([res_dict['TPR'], res_dict['TNR']])\n",
    "    res_dict['GM'] = np.sqrt(res_dict['TPR']*res_dict['TNR'])\n",
    "    res_dict['F1(Macro)'] = sm.f1_score(y, p, average='macro')*100\n",
    "    res_dict['F1(Micro)'] = sm.f1_score(y, p, average='micro')*100\n",
    "    res_dict['Acc'] = sm.accuracy_score(y, p)*100\n",
    "    return res_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_dict(d):\n",
    "    list_final = []\n",
    "    for i in d:\n",
    "        list_now = i + '_' + str(d[i])\n",
    "        list_final.append(list_now)\n",
    "    return list_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    data_name, k_holdout, k_cv, pars = run_Config()\n",
    "    all_fea, all_label = run_load_data(data_name)\n",
    "    res_list, opt_pars_list = run_doubleCV(all_fea, all_label, k_holdout, k_cv, pars)\n",
    "    # print 'Each heldout cv result:'\n",
    "    # print '-----------------------'\n",
    "    for i, j in zip(res_list,opt_pars_list):\n",
    "        print get_dict(i), 'with hyper-parameters:'\n",
    "        print get_dict(j)\n",
    "        print '-----------------------'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
