{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/tensorflow/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Basic models\n",
    "import traceback\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Data operation models\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import cross_validation  \n",
    "from sklearn.model_selection import ParameterGrid \n",
    "\n",
    "# Classifier models\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Evaluation models\n",
    "import sklearn.metrics as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Configurations for the model\n",
    "def run_Config():\n",
    "    data_name = 'Thoracic_Surgery_onehot.xlsx'\n",
    "    k_holdout = 10\n",
    "    k_cv = 5\n",
    "    origin_pars = {\n",
    "            'indicator_name': ['macc'],\n",
    "            'k_smote': [1,3,5], # smote_hyper-par\n",
    "            'r_newpoint': [1,2,3], # smote_hyper-par\n",
    "            'n_estimators': [5,10,30,50],\n",
    "            'max_depth': [2,4,6,8],\n",
    "            'max_features': ['log2'],\n",
    "            'class_weight':[{1:0.1,2:0.9},{1:0.2,2:0.8},{1:0.25,2:0.75},{1:0.4,2:0.6}]\n",
    "            }\n",
    "    pars = list(ParameterGrid(origin_pars))\n",
    "    return data_name, k_holdout, k_cv, pars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "def run_load_data(data_name):\n",
    "    all = pd.read_excel(data_name)\n",
    "    all = all.values\n",
    "    all_fea = all[:,:-1]\n",
    "    all_label = get_normal_label(all[:,-1])\n",
    "    return all_fea, all_label\n",
    "\n",
    "def get_normal_label(y):\n",
    "    y_uni = np.unique(np.array(y))\n",
    "    for i in xrange(len(y_uni)):\n",
    "        y[np.nonzero(y == y_uni[i])[0]] = i+1\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Run both HoldoutCV and GridSearchCV\n",
    "def run_doubleCV(all_fea, all_label, k_holdout, k_cv, pars):\n",
    "    # HoldoutCV\n",
    "    i_t = 0\n",
    "    res_list = []\n",
    "    opt_pars_list = []\n",
    "    holdoutcv = StratifiedKFold(n_splits = k_holdout, shuffle = True)\n",
    "    for i_learn, i_test in holdoutcv.split(all_fea, all_label):\n",
    "        # Times\n",
    "        i_t = i_t + 1\n",
    "        # print 'Round ', str(i_t), ' Holdout CV----------------------'\n",
    "        \n",
    "        # Obtain current learning and heldout data\n",
    "        learn_fea, test_fea = all_fea[i_learn], all_fea[i_test]\n",
    "        learn_label, test_label = all_label[i_learn], all_label[i_test]\n",
    "        \n",
    "        # GridSearchCV\n",
    "        j_t = 0\n",
    "        optdata = {'score':0}\n",
    "        for i_pars in pars:\n",
    "            # times\n",
    "            j_t = j_t + 1\n",
    "            # print 'round ', str(j_t), ' gridsearch cv----------------'\n",
    "            pars_score = []\n",
    "            gridcv = StratifiedKFold(n_splits = k_cv, shuffle = True)\n",
    "            for i_train, i_valid in gridcv.split(learn_fea, learn_label):\n",
    "                # obtain current training and validation data\n",
    "                train_fea, valid_fea = learn_fea[i_train], learn_fea[i_valid]\n",
    "                train_label, valid_label = learn_label[i_train], learn_label[i_valid]\n",
    "                # learn the model\n",
    "                # i_pars = {'par_name1':par1,'par_name2':par2,...,'par_nameN':parN}\n",
    "                valid_pre, pars_new = run_smote_model(i_pars, train_fea, train_label, valid_fea, 0)\n",
    "                grid_score = run_validation(valid_pre, valid_label, i_pars['indicator_name'])\n",
    "                pars_score.append(grid_score)\n",
    "            if np.mean(pars_score) > optdata['score']:\n",
    "                optdata['pars'] = pars_new\n",
    "                optdata['score'] = np.mean(pars_score)\n",
    "                    \n",
    "        # Holdout testing\n",
    "        # best_pars is a dict too\n",
    "        best_pars = optdata['pars']\n",
    "        test_pre, _ = run_smote_model(best_pars, learn_fea, learn_label, test_fea, 1)\n",
    "        \n",
    "        # Evaluate the prediction\n",
    "        res_now = run_evaluation(test_pre, test_label)\n",
    "        \n",
    "        # Save results\n",
    "        res_list.append(res_now)\n",
    "        opt_pars_list.append(best_pars)\n",
    "        \n",
    "    return res_list, opt_pars_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Fit the model and make the prediction\n",
    "def run_smote_model(p, X, y, Z, i_tag):\n",
    "    if i_tag == 0:\n",
    "        # run the smote to get new training data\n",
    "        X_new, y_new, k_new, r_new = run_SMOTE(X, y, p['k_smote'], p['r_newpoint'])\n",
    "        # unpdate k_smote and r_newpoint\n",
    "        p['k_smote'] = k_new\n",
    "        p['r_newpoint'] = r_new\n",
    "    else:\n",
    "        X_new, y_new = X, y\n",
    "    \n",
    "    # run basic model\n",
    "    clf = RandomForestClassifier(n_estimators=p['n_estimators'], \n",
    "                                 max_depth=p['max_depth'], \n",
    "                                 max_features=p['max_features'], \n",
    "                                 class_weight=p['class_weight'])\n",
    "    clf.fit(X_new,y_new)\n",
    "    zpre = clf.predict(Z)\n",
    "    # print 'SMOTE-RF is running...'\n",
    "    return zpre, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Oversampling by SMOTE\\ndef run_SMOTE(X, y, k, r):\\n    return X, y, k, r'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# Oversampling by SMOTE\n",
    "def run_SMOTE(X, y, k, r):\n",
    "    return X, y, k, r\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Oversampling by SMOTE\n",
    "from sklearn.neighbors import NearestNeighbors as kNN\n",
    "def run_SMOTE(X, y, k, r):\n",
    "    \n",
    "    # obtain postive and negative data\n",
    "    v = np.unique(y)\n",
    "    if len(v) == 2:\n",
    "        if sum(y == v[0]) >= sum(y == v[1]):\n",
    "            X_pos = X[y == v[1],:]\n",
    "            X_neg = X[y == v[0],:]\n",
    "            y_pos = y[y == v[1]]\n",
    "            y_neg = y[y == v[0]]\n",
    "        else:\n",
    "            X_pos = X[y == v[0],:]\n",
    "            X_neg = X[y == v[1],:]\n",
    "            y_pos = y[y == v[0]]\n",
    "            y_neg = y[y == v[1]]\n",
    "    else:\n",
    "        raise Exception(\"Not a binary-class!\")    \n",
    "    n_pos = X_pos.shape[0]\n",
    "    n_neg = X_neg.shape[0]\n",
    "    \n",
    "    # constrain hyper-parameters to their suitable ranges\n",
    "    if n_pos == 0:\n",
    "        raise Exception(\"No positive samples!\")\n",
    "    elif k + 1 > n_pos:\n",
    "        k = n_pos - 1\n",
    "    else:\n",
    "        while k > 1:\n",
    "            while r > 1:\n",
    "                if n_pos + n_pos*k*r >= n_neg:\n",
    "                    r = r - 1\n",
    "                else:\n",
    "                    break   \n",
    "            if n_pos + n_pos*k*r >= n_neg:\n",
    "                k = k - 1\n",
    "            else:\n",
    "                break\n",
    "    \n",
    "    # find k nearest neighbors of each positive sample\n",
    "    kNN_model = kNN(n_neighbors=k+1)\n",
    "    kNN_model.fit(X_pos)\n",
    "    i_neighbor = np.argsort(kNN_model.kneighbors(X_pos, return_distance = False), axis = 1)\n",
    "    i_neighbor = i_neighbor[:,1:]\n",
    "    \n",
    "    # generate new positive samples and corresponding labels\n",
    "    new_pos_mat = []\n",
    "    for i in xrange(n_pos):\n",
    "        now_sample = X_pos[i,:]\n",
    "        for j in xrange(k):\n",
    "                new_sample_mat = get_new_pos(now_sample, X_pos[i_neighbor[i,j],:], r)\n",
    "                new_pos_mat.extend(new_sample_mat)\n",
    "    new_pos_label = y_pos[0] * np.ones((len(new_pos_mat)))\n",
    "    \n",
    "    # combine the newly-generated ones to the original data\n",
    "    X_new = np.concatenate((X_neg, X_pos, new_pos_mat), axis = 0)\n",
    "    y_new = np.concatenate((y_neg, y_pos, new_pos_label), axis = 0)                                 \n",
    "    i_shuffle = np.random.permutation(len(y_new))\n",
    "    X_new = X_new[i_shuffle, :] \n",
    "    y_new = y_new[i_shuffle] \n",
    "    \n",
    "    return X_new, y_new, k, r\n",
    "\n",
    "def get_new_pos(point, neighbor, r):\n",
    "    # Function to generate new r samples according to point and one of its neighbor\n",
    "    new_points = []\n",
    "    for i_r in xrange(r):\n",
    "        new_point = point + (point - neighbor) * np.random.rand(point.shape[0])\n",
    "        new_points.extend(np.array([new_point]))\n",
    "    return new_points         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Obtain the score\n",
    "def run_validation(zpre, z, s_name):\n",
    "    i_pos = np.nonzero(z == max(z))\n",
    "    i_neg = np.nonzero(z == min(z))\n",
    "    tpr = (1 - sm.hamming_loss(z[i_pos], zpre[i_pos]))*100\n",
    "    tnr = (1 - sm.hamming_loss(z[i_neg], zpre[i_neg]))*100\n",
    "    if s_name.lower() == 'macc':\n",
    "        s = 0.5*(tpr+tnr)\n",
    "    elif s_name.lower() == 'gm':\n",
    "        s = np.sqrt(tpr*tnr)\n",
    "    elif s_name.lower() == 'tpr':\n",
    "        s = tpr\n",
    "    else: # error\n",
    "        s = sum((1 if i_pre == i_true else 0 for i_pre, i_true in zip(zpre,z)))/float(len(z))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Evaluate the performance\n",
    "def run_evaluation(p, y):\n",
    "    res_dict = {}\n",
    "    i_pos = np.nonzero(y == max(y))\n",
    "    i_neg = np.nonzero(y == min(y))\n",
    "    res_dict['TPR'] = (1 - sm.hamming_loss(y[i_pos], p[i_pos]))*100\n",
    "    res_dict['TNR'] = (1 - sm.hamming_loss(y[i_neg], p[i_neg]))*100\n",
    "    res_dict['MAcc'] = np.mean([res_dict['TPR'], res_dict['TNR']])\n",
    "    res_dict['GM'] = np.sqrt(res_dict['TPR']*res_dict['TNR'])\n",
    "    res_dict['F1(Macro)'] = sm.f1_score(y, p, average='macro')*100\n",
    "    res_dict['F1(Micro)'] = sm.f1_score(y, p, average='micro')*100\n",
    "    res_dict['Acc'] = sm.accuracy_score(y, p)*100\n",
    "    return res_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_dict(d):\n",
    "    list_final = []\n",
    "    for i in d:\n",
    "        list_now = i + '_' + str(d[i])\n",
    "        list_final.append(list_now)\n",
    "    return list_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    data_name, k_holdout, k_cv, pars = run_Config()\n",
    "    all_fea, all_label = run_load_data(data_name)\n",
    "    res_list, opt_pars_list = run_doubleCV(all_fea, all_label, k_holdout, k_cv, pars)\n",
    "    # print 'Each heldout cv result:'\n",
    "    # print '-----------------------'\n",
    "    for i, j in zip(res_list,opt_pars_list):\n",
    "        print get_dict(i), 'with hyper-parameters:'\n",
    "        print get_dict(j)\n",
    "        print '-----------------------'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
